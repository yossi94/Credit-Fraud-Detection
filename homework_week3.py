# -*- coding: utf-8 -*-
"""Homework_week3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C6wCqL6nG9fTSuwAXiXHQ3SDZe-PhwXX

# Setup and Data Import
"""

import pandas as pd
import numpy as np

id_raw_df =pd.read_csv("train_identity.csv")
id_trans_df =pd.read_csv("train_transaction.csv")

"""# Data Preparation and Exploration"""

import matplotlib.pyplot as plt
import seaborn as sns

id_raw_df.shape

id_trans_df.shape

train_full_df = pd.merge(id_trans_df, id_raw_df, on='TransactionID', how='left')

train_full_df.shape

len(train_full_df[train_full_df["isFraud"]==1])/len(train_full_df)*100

"""# NA Values"""

plt.figure(figsize=(90,40))
sns.barplot(y=train_full_df.isnull().sum().sort_values(ascending=False)/len(train_full_df),
            x=train_full_df.isnull().sum().sort_values(ascending=False).index,
            palette="Reds_d")
plt.title("Percent Missing Value",size=50)
plt.xticks(rotation=90)

"""# Data Preparation"""

from sklearn.model_selection import train_test_split

"""### Handling NAs:

### Strategies for dealing with NAs:



*   Drop Data
*   Impute/recode NAs
"""

columns_def=pd.DataFrame({"na_count":train_full_df.isnull().sum().sort_values(ascending=False)})

#create column of percent columns, #Higher is worst
columns_def["per"]=columns_def["na_count"]/len(train_full_df)

#cycle through various limit for percent Na per column 
range=[0.05, 0.10, 0.15, 0.25, 0.40, 0.60, 0.75]

for i in range:
  print (columns_def[columns_def["per"]<i].shape[0]/train_full_df.shape[1])

limit=0.10 #only columns w/ less 10% will remain

#select final columns
columns_final=columns_def[columns_def["per"]<limit].index


train_full_df=train_full_df[train_full_df.columns.intersection(columns_final)]

"""# Dealing with Ctegorical Variables"""

#show categorical variables
train_full_df[train_full_df.select_dtypes(include=['object']).columns]

card6_dummy_df=pd.get_dummies(train_full_df["card6"])

train_full_df.shape

train_full_df=pd.concat([train_full_df,card6_dummy_df],axis=1)

train_full_df=train_full_df._get_numeric_data()
train_full_df=train_full_df.dropna()

len(train_full_df[train_full_df["isFraud"]==1])/len(train_full_df)*100

x_full_df=train_full_df.drop(["isFraud"],axis=1)
y_full_df=train_full_df["isFraud"]

X_train,X_test,y_train,y_test=train_test_split(x_full_df,y_full_df,test_size=0.50)

"""# Modelling"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score

lr = LogisticRegression(solver='lbfgs')
lr.fit(X_train, y_train)

lr.score(X_test, y_test)

y_pred=lr.predict(X_test)

pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

recall_score(y_test, y_pred)

accuracy_score(y_test, y_pred)

recall_score(y_test, y_pred)